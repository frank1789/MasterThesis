\section{Datatset}
\label{sec:dataset}
Particular attention is needed in the construction of a good training dataset, 
in fact, as seen before in (\ref{subsec:supervised-learnig}), we deal with a 
supervised learning where we know the response of our labels and bounding box position's objects.
A script is provided that can build a dataset divided into folders: training, 
validation and testing; as you can see in figure \ref{fig:datasetstructure}.
A large number of figures per sample that clearly highlights the characteristics 
that you want to study allows a greater rate of success of the training 
preventing the \emph{overfitting}.
Acquiring a large number of images is not always achievable, using some 
augmentation techniques, virtually allows to increase the observability of a 
images, for example, by rotating, distorting and translating them.
The two train and validated folders are essential for the addition of the 
neural network.
Instead, the test folder contains a set of images that the network has never 
seen and so necessary to measure the degree of confidence acquired in the network.\linebreak
%
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.75\textwidth]{structure.png}
	\caption{Dataset structure}
	\label{fig:datasetstructure}
\end{figure}
%
\subsection{Landing zone dataset}
\label{ssec:landing-zone}
%
The dataset was artificially constructed with a 3D Blender graphics program. 
The object of interest are the drone landing mats. These have different shapes
and colors in fact they are available in various color shapes. The signs on
these also vary from the classic H to X to more or less conspicuous symbols.
Thus three models were created, two with a circular plan and one with a square
plan. Textures were then applied to these two models to obtain a faithful
representation of that of concrete objects. After the construction of the carpet
models, it was necessary to contextualise them in credible scenarios. \hfill \break
Thus,
three main scenarios were created: first scenario placed in the middle of a road
junction. Second scenario near a straight road flanked by a side-walk.
Third zone is set in the countryside. To take the shots, some photographic
factors were taken into account, in fact they are generated starting from the
technical characteristics of the Raspberry camera presented in section
(\ref{sec:raspicam}). Moreover, thanks to a simple script, the trigger points
were generated with respect to the landing mat position, these positions vary in
height, width and depth with respect to the carpet placed on the ground.
A scheme is visible in figures (\ref{fig:poi_dataset}). \hfill \break
%
\begin{figure}[!h]
	\centering
	\includegraphics[width=0.75\textwidth]{poi_dataset.png}
	\caption{Configuration camera position.}
	\label{fig:poi_dataset}
\end{figure}
%
\\To increase the variety of rendered shots, a very basic day-night cycle was
used to detect the color variation. All the shots are taken with a top view as
seen in the examples shown in (\ref{fig:renders}).
After completing the shots they annotated themselves highlighting the position
of the object of interest in the various shots and positions.
The process is carried out with the aid of a software that extracts and
generates the annotations. as shown in figure (\ref{fig:annotation}).
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.75\textwidth]{annotation.png}
	\caption{Process of annotation.}
	\label{fig:annotation}
\end{figure}
%
\newpage
\begin{figure}[htb]
    \centering
    \subfloat[][\emph{middle of a road junction (sunrise)}.\label{subfig:junction}]
        {\includegraphics[width=.50\textwidth]{result1.jpg}} \quad
    \subfloat[][\emph{countryside}.\label{subfig:country}]
        {\includegraphics[width=.50\textwidth]{result2.jpg}} \quad
    \subfloat[][\emph{close position at the crossroads}.\label{subfig:junction2}]
        {\includegraphics[width=.50\textwidth]{result3.jpg}}
    \caption{Examples shots scenarios after render}
    \label{fig:renders}
\end{figure}
%
\newpage
\subsection{Thermal imaging dataset}
\label{ssec:thermal-image-dataset}
The dataset created and distributed free of charge by
FLIR\footnote{\href{https://www.flir.com/oem/adas/adas-dataset-form/}{https://www.flir.com/oem/adas/adas-dataset-form/}}
was used to train the neural network on the recognition and classification of
objects in thermal images. The ability to sense thermal infrared radiation, or
heat, within the ADAS context provides both complementary and distinct
advantages to existing sensor technologies such as visible cameras, Lidar and
radar systems: With over 15 years of experience in automotive, FLIR has the only
automotive-qualified thermal sensor that is deployed in over 500,000 cars today
for driver warning systems. The FLIR thermal sensors can detect and classify
pedestrians, bicyclists, animals and vehicles in challenging conditions
including total darkness, fog, smoke, inclement weather and glare, providing a
supplemental dataset beyond LiDAR, radar and visible cameras. The detection
range is four times farther than typical headlights. When combined with visible
light data and distance scanning data from LiDAR and radar, thermal data paired
with machine learning creates a more comprehensive detection and classification
system.\cite{flirdataset}
%
\begin{figure}[htb]
    \centering
    \subfloat[][\emph{person}.\label{subfig:th-person}]
        {\includegraphics[width=.50\textwidth]{FLIR_00658.jpeg}} \quad
    \subfloat[][\emph{cars and people}.\label{subfig:th-car}]
        {\includegraphics[width=.50\textwidth]{FLIR_00690.jpeg}} \quad
    \subfloat[][\emph{bycicle and persons}.\label{subfig:th-bycicle}]
        {\includegraphics[width=.50\textwidth]{FLIR_00356.jpeg}}
    \caption{Thermal image extrated from FLIR dataset.}
    \label{fig:th-image}
\end{figure}