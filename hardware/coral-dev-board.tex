\section{Coral Dev-Board}
\label{sec:hard-devboard}
The Coral Dev Board is a single-board computer that contains an Edge TPU
coprocessor. It's ideal for prototyping new projects that demand fast on-device
inferencing for machine learning models.
%
\begin{figure}[htb]
	\centering
    \includegraphics[width=0.80\textwidth]{devboard-dimensions.jpg}
    \captionsource{Coral Dev Board.}{
    \href{https://coral.ai/docs/dev-board/get-started/}{https://coral.ai/docs/dev-board/get-started/}}
    \label{fig:dev-board}
\end{figure}
%
\subsection{Overview}
\label{ssec:hard-devboard-overview}
The Coral Dev Board is a single-board computer that's ideal when you need to
perform fast machine learning (ML) inferencing in a small form factor. You can
use the Dev Board to prototype your embedded system and then scale to production
using the on-board Coral System-on-Module (SoM) combined with your custom PCB
hardware. The SoM provides a fully-integrated system, including NXP's iMX 8M
system-on-chip (SoC), eMMC memory, LPDDR4 RAM, Wi-Fi, and Bluetooth, but its
unique power comes from Google's Edge TPU coprocessor. The Edge TPU is a small
ASIC designed by Google that provides high performance ML inferencing with a low
power cost. For example, it can execute state-of-the-art mobile vision models
such as MobileNet v2 at almost 400 FPS, in a power efficient manner. 
The baseboard provides all the peripheral connections you need to prototype a
project, including USB 2.0/3.0 ports, DSI display interface, CSI-2 camera
interface, Ethernet port, speaker terminals, and a 40-pin I/O header. \hfill \break
Key benefits of the Dev Board:
\begin{itemize}
	\item High-speed and low-power ML inferencing (4 TOPS @ 2 \si{\watt})
	\item A complete Linux system (running Mendel, a Debian derivative)
	\item Prototyping and evaluation board for the small Coral SoM (40 x 48 \si{\milli\meter})
\end{itemize}
%
\begin{table}[htb]
	\centering
	\begin{tabular}{c}
	\hline \\
Edge TPU System-on-Module (SoM)\\
% NXP i.MX 8M SoC (Quad-core Arm Cortex-A53, plus Cortex-M4F)
% Google Edge TPU ML accelerator coprocessor
% Cryptographic coprocessor
% Wi-Fi 2x2 MIMO (802.11b/g/n/ac 2.4/5 GHz)
% Bluetooth 4.2
% 8 GB eMMC
% 1 GB LPDDR4
% USB connections
% USB Type-C power port (5 V DC)
% USB 3.0 Type-C OTG port
% USB 3.0 Type-A host port
% USB 2.0 Micro-B serial console port
% Audio connections
% 3.5 mm audio jack (CTIA compliant)
% Digital PDM microphone (x2)
% 2.54 mm 4-pin terminal for stereo speakers
% Video connections
% HDMI 2.0a (full size)
% 39-pin FFC connector for MIPI DSI display (4-lane)
% 24-pin FFC connector for MIPI CSI-2 camera (4-lane)
% MicroSD card slot
% Gigabit Ethernet port
% 40-pin GPIO expansion header
% Supports Mendel Linux (derivative of Debian)
\hline
	\end{tabular}
	\caption{Coral Dev Board Features.}
	\label{tab:hard-devboard-spec}
\end{table}
%
\noindent The Google Coral with a special TPU chip
performing all tensor calculationsworks with special 
pre-compiled TensorFlow Lite networks. If the topology of the neural network and
its required operations can be described in TensorFlow it may work well on the
Google Coral. However, with its sparse 1 Gbyte RAM, memory shortage can still be
an issue. The Google USB accelerator has its special back-end compiler
converting a TensorFlow Lite file to an executable model for the dongle
TPU.
% 
%
\section{TPU explained in depth}
\label{sec:hard-tpu}
% cite page https://qengineering.eu/google-corals-tpu-explained.html
The Google Coral has a TPU on board which speeds up the tensor calculations
enormously. These tensor calculations are used in deep learning and neural
networks. \hfill \break 
The Google Coral Dev board use an ASIC made by the Google team called the Edge 
TPU.  It is a much lighter version of the well-known TPUs used in Google's 
datacenter. \hfill \break
It also consumes very little power, so it is ideal for small embedded systems. 
Nevertheless, the similarities in applied technology are significant.\cite{TPU:explained} 
%
\subsection{Neural Node}
\label{ssec:hard-neural-node}
Neural networks used in deep learning consists of many neural nodes.  They are
all connected together in a defined way. The way these nodes are wired  is
called the topology of the network.  This topology determines the function the
network performs. See this list for a  selection of several types of deep
learning networks. Each node has always three basic components. 
A multiplier multiplies all the  inputs with their respective so-called weight,
the synapses. An adder who accumulates all the individual multiplications.  And
an activation function that shapes the output given the addition. A schematic
view below (\ref{fig:neuron}).\hfill \break
%
\begin{figure}[htb]
	\centering
    \includegraphics[width=0.70\textwidth]{sinapse.jpg}
    \captionsource{Artificial Neuron models and its parts.}{Adapted from Haykin (1994)}
    \label{fig:neuron}
\end{figure}
%
\newpage
\noindent The formula can be written as follows \eqref{eq:neuron}.
\begin{equation}
\label{eq:neuron}
	y_{i} = \phi \, \left(\sum_{i=1}^{n} w_{ij}\cdot x_{i} \right)
\end{equation}
%
Deep neural network is consist of millions of neural nodes distribute over many
layers; despite the simplicity of the operation, only on multiplication and one
addition, require long compute time to obtain a result. keep in mind that for
training a network requires many epochs. In other words, the main problem is
time. The algorithms is well suited for parallel execution. Although
distributing  the algorithm over the processes and threads can be an option
actually  provides disappoint results for the presence of bottlenecks due to
architecture general purpose. On the other hand use of Graphical Processing
Unit,  the GPU on video card designed for efficient manipulation memory. Their
highly parallel structure make them more efficient respect CPU especially for
algorithms that process large blocks of data in parallel. The best choice is the
use of the Tensor Processing Unit, the TPU. This device has been specially
designed for the above neural node algorithm.
%
\subsection{The adder}
\label{ssec:hard-adder}
Generally a strategy adopted when software is tool slow is modify the hardware
to reach the better achievements.
The structure inside TPU is realized by three main components derived from 
neural node, then keeping in mind the scheme in figure (\ref{fig:neuron}): the 
\textbf{multiplier}, the \textbf{adder} and the \textbf{activation function} 
must be included in the hardware.
Start with analysing the diagram of the 4-bit adder realized in figure
(\ref{fig:4-bit-adder}).
Where: \texttt{A} and \texttt{B} are the inputs. If the output overflows 
\texttt{C4} the carry out is set. \texttt{C0} is the carry-in of a previous 
phase.\cite{TPU:explained}
%
\begin{figure}[!h]
	\centering
    \includegraphics[width=0.75\textwidth]{Adder1.png}
    \captionsource{4-bit adder}{\href{https://qengineering.eu/google-corals-tpu-explained.html}{Q-engineering}}
    \label{fig:4-bit-adder}
\end{figure}
%
Signals \texttt{A} and \texttt{B} propagate through the circuit and generate the
result \texttt{A + B}. Changing one of them alters almost immediately the
output. This happens extremely very fast, within a few nanoseconds. This
propagation time depends on the number of digital ports whose output changes.
The propagation time is therefore not fixed, but lies between two limits, a
minimum and a maximum time, see diagram at the bottom of the drawing
(\ref{fig:4-bit-adder}).  By the way, al mentioned times are illustrative and
have no relationship to any device.\cite{TPU:explained}
%
\newpage
\subsection{Pipeline}
\label{ssec:hard-pipeline}
Consider the example where: the propagation time of one adder is $2 \,
\si{\nano\second}$, therefore the maximum clock rate can be $500
\,\si{\mega\hertz}$. Taking into account that a neural node can have many
inputs, also hundreds, that must be accumulated this affect the propagation time
that increase dramatically. This implies that the last adder in the chain must
be attend all intermediate result before its output becomes stable.
If we consider a chain of 250 input each of one with $2\, \si{\nano\second}$
delay, we obtain total time of $500 \,\si{\nano\second}$ waiting. \\
Consequently clock results extremely slow $2\,\si{\mega\hertz}$, then solution adopted
is structured pipeline where between each adder is inserted a memory element
that keep keeps the result stable for the next adder. 
As show in figure (\ref{fig:4-bit-adder-register}). \hfill \break
%
\begin{figure}[!h]
	\centering
    \includegraphics[width=0.75\textwidth]{AdderReg10.png}
    \captionsource{4 bit adder with register}{\href{https://qengineering.eu/google-corals-tpu-explained.html}{Q-engineering}}
    \label{fig:4-bit-adder-register}
\end{figure}
%
\break The output of the registers is updated at the rising edge of the clock signal.
That is the only time the output can change. When the clock signal is high or
low, the inputs cannot manipulate the output, then it remains stable.  
Just before the clock rises, the input must be stable for a minimum time, also just after the rise time. 
In contrast to the previous example, now consider a delay of $0.1 \,\si{\nano\second}$ and adding the register's propagation time of $(0.4 \,\si{\nano\second})$, the total propagation reach for a
new stable signal is now $2.5 \si{\nano\second}$, which results increasing up to clock speed of $400 \,\si{\mega\hertz}$. Observable in figure (\ref{fig:pipe}).
%
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.60\textwidth]{pipe10.png}
	\captionsource{Digital pipeline.}{\href{https://qengineering.eu/google-corals-tpu-explained.html}{Q-engineering}}
	\label{fig:pipe}
\end{figure}
%
%
Taking into account the table in figure \ref{fig:pipe}, each color represents a value. 
After four clock cycles, the value at the input has propagated through the network and appears at the output.  
Because the registers are updated simultaneously a new input is accepted every
$2.5 \,\si{\nano\second}$.  
The propagation time has been restored. 
The time it takes to travel through the whole pipeline is called latency time
and is $10 \,\si{\nano\second}$ in the above diagram $(4 \times 2.5
\,\si{\nano\second}$). 
Every digital component is built on large pipelines which guarantee the required
speed.\cite{TPU:explained}   
%
\subsection{The mul-add cell}
\label{ssec:The-mul-add-cell} 
The input signal of every neural node has effect on final result.
This gives a simple multiplication for an input mediate by a weight value. 
The operation of multiplication can be implemented with same logic of an adder, then are necessary more gates.
Remembering that the neural node has multiplied his value for weight value. Thus the representation assume the scheme in figure (\ref{fig:mul-add-register}). \hfill \break
%
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.65\textwidth]{MulAddReg10.png}
	\captionsource{Mul-Add register.}{\href{https://qengineering.eu/google-corals-tpu-explained.html}{Q-engineering}}
	\label{fig:mul-add-register}
\end{figure}
%
%
%\break For the sake of simplicity, no additional propagation time fixing registers have
%been added. With the formula of the neural node in mind, it is now relatively
%easy to design such a neuron with a few mul-add cells. Below the schematic for a
%three input neuron.
%
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.75\textwidth]{Neuron.png}
	\captionsource{schematic for a
three input neuron.}{\href{https://qengineering.eu/google-corals-tpu-explained.html}{Q-engineering}}
	\label{fig:3-input-neuron}
\end{figure}
%
%
\newline
Are necessary many important clarifications about the input registers \texttt{X1} and \texttt{X2} displayed in figure (\ref{fig:3-input-neuron}).
They generate a delay line in the \emph{mul-add} chain, thus can permit to correct synchronize.
Considering the second cell \emph{mul-add} who sum the output from previous cell (signal \texttt{A}) retarded by one clock cycle.
Consequently also the multiplication between the value in register \texttt{X1} and weight value must be delayed the same amount of time.
The schema reported in figure (\ref{fig:3-input-neuron}) shows for each colour that represent the vector of input \texttt{$[X0, X1, X2]$} while the input \texttt{A} and \texttt{B}, represented by lines straight, maintains always same colour. 
This means that they appear simultaneous, i.e. are synchronised.
The same must be valid for input \texttt{C} and \texttt{D}.
%
\subsection{Systolic array}
\label{ssec:hard-systolic-array}
Starting from structure, just examined, is quite easy extend it to other neurons, taking into account that every single input is connected with all neurons in the next layer, as shows in figure (\ref{fig:systolic-array-edge-tpu}).
%
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.65\textwidth]{Systolic-Array.png}
	\captionsource{Systolic Array Edge TPU.}{\href{https://qengineering.eu/google-corals-tpu-explained.html}{Q-engineering}}
	\label{fig:systolic-array-edge-tpu}
\end{figure}
%
%
The adoption of this solution, called systolic array, permits to have an increment of speed in parallel calculation. 
Since the propagation time remains unchanged for an spread of systolic array in depth or width, while only the latency time grows up this represent the solution widely used for neural network hardware.
%
%The sizes of the systolic array in the Edge TPU are not known yet. The first TPU
%Google uses in its datacenter contains 256 x 256 \emph{mul-add} cells. Running at 700
%MHz it can theoretical performs 256 x 256 x 700.000.000 = 46 trillion \emph{mul-adds}
%per second. Or if you look to individual operations 92 trillion (92 TOPS).
%However, this impressive figure is purely theoretical. In practice, there are
%some factors that slow performance.
%
%The systolic array is made up of hardware. This means that it has fixed
%dimensions. Therefore, the input and output vectors also have fixed dimensions.
%However, the numbers of neurons per layer are determined by the design and are
%certainly not constant. If the number is smaller than the size of the array, it
%is, of course, possible to expand an input vector with leading zeros, so that
%the dimension equals the array size. The same technique can be applied to an
%oversized output vector.
%
%If the input vector contains more elements than the width of the systolic array,
%the entire arithmetic calculation must be performed in more than one call. To
%this end, the input vector is cut into a series of smaller vectors that match
%the width of the series. They are then processed one after the other. The
%intermediate results must be stored in a buffer. These values must be added
%after completion of all the systolic calls. Note that not only the input vector
%is split into smaller parts, but all weights used in the multiplication must be
%updated according to the formula's processing section. This may require a
%massive memory transfer. Below an example of a systolic array with only four
%inputs processing a vector of size 12.
%
%
%
%Looking at Google's schematic overview of the TPU, these output buffers and
%accumulators are drawn at the bottom. (In their diagram, the output is at the
%bottom instead of the right as on our drawing).
%Google TPU detail
%
%  
%Buffers have also been placed on the input vector side in the diagram above.
%They act as a first-in-first-out buffer, a FIFO. This guarantees continuous
%input of the systolic array with input values. It may also have some rotating
%operations, which increases the performance of CNN networks. It is unclear
%whether Google uses this method here.  

\subsection{Activation unit}
\label{ssec:activation-unit}
Once the output is available, it is sent to the activation unit. This module within the Edge TPU applies the activation function to the output. It is hardwired. In other words, you cannot alter the function, it works like a ROM. Probably it is a ReLU function as it is nowadays the most used activation function and it is very easy to implement in hardware.\cite{TPU:explained}
\newpage
 
TPU versus Edge TPU.
The Google's Edge TPU is a much smaller device than the TPUs that Google uses in its data centers. Below the PCB with the original TPU and the Edge TPU on the same scale.
TPU vs Edge TPU
It is obvious that such a small device cannot have the same functions as its much larger ancestor. The floor plan on the die does not allow this to happen. Nor can the systolic array have the $256 \times 256$ dimension as in the original TPU. Google has so far not revealed the measures in the Edge, but a well-founded estimate is $64 \times 64$ with a clock of $480 \si{\mega\hertz}$, resulting in 4 TOPS.
Memory is also an issue. In the original chip, it takes around $29\%$ of the total floor plan. It is impossible that the same amount can be found on the Edge TPU die. Moreover, the chip is very energy efficient. It requires no cooling. This means that almost certainly all the buffering is outside the chip. It leaves only a simple transfer of input vectors, output vectors and weights to the chip.  
Practice.
In fact, that is the whole concept of the Google Coral Dev board. Transfer all tensor calculations to the TPU as quickly as possible, fetch for the results, fill them in the next layer and start the cycle again with this layer. Process all layers one by one until they are ready.
To speed up the process, TensorFlow uses a special back end compiler, the Edge TPU compiler. The main task of this Edge TPU compiler is to partition the TensorFlow Lite file into more suitable TPU transfer packages. If for any reason the TPU cannot process the TensorFlow Lite file or part of it, the CPU will take care of it. However, if this happens, your application will, of course, work extremely slowly.
8 bits integers.
Another way of speeding up the calculations is the use of 8 bit signed integers instead of floats. A floating number occupies 32 bits in memory whereas the integer only needs 8. Because a neural network is fairly insensitive to number accuracy, it will also work well with 8-bit integers. It will still remain accurate. Not only does this technology save approximately $75\%$ memory, but it also significantly reduces the number of transistors on the chip. If you look at the adder at the top of the page, you can imagine how many transistors a floating-point adder needs. Without this compression, the Edge TPU would not be as powerful, small and energy efficient.

The converting algorithm from floating point to 8 bit signed integer is quite simple. First, it determines which maximum and minimum a variable will take in the model. The larger of the two is then taken and scaled to 127. Suppose minimum is -1205.8 and maximum is 646.8. The larger is the min value of 1205.8. So that number becomes -127. Zero remains zero. That gives 646.8 a integer value of 127 x (646.8 / 1205.8)  = 68. Every number in the TensorFlow model is converted in this way. Not only for the Edge TPU, by the way. All TensorFlow Lite models for embedded deep learning are processed in this way.

A rather remarkable detail is the output of the Edge TPU, which does consist of a floating point.
Because the activation function is embedded in a ROM, it is just as easy to store floating numbers as integers.
One last tip in this regard. Apply quantization-aware training in TensorFlow. This simulates the effect of the 8 bit signed integers. Therefore generating a more precise model. It makes the model also more tolerant for low precision variables.